{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jnoxon/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:43: DeprecationWarning: This module has been deprecated in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_columns = 999\n",
    "from IPython.core.display import HTML\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import datetime as dt\n",
    "from datetime import timedelta\n",
    "import itertools\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import RandomizedLasso\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import sklearn.cross_validation\n",
    "from sklearn import discriminant_analysis\n",
    "from sklearn import naive_bayes\n",
    "from sklearn import tree\n",
    "from sklearn import cluster\n",
    "from sklearn import metrics\n",
    "from sklearn import cross_validation\n",
    "from sklearn import svm\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "%matplotlib inline\n",
    "pd.options.display.max_columns = 999\n",
    "\n",
    "from os.path import expanduser\n",
    "home = expanduser(\"~\")\n",
    "path = home + \"/Dropbox/python/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.auto_scroll_threshold = 9999;"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".container { \n",
       "\n",
       "  width:100% !important;\n",
       "  background-color: #050505 !important;\n",
       " }\n",
       "/*body {background-color: #1b202b;}*/\n",
       "\n",
       "\n",
       "/* GLOBALS */\n",
       "body {background-color: #050505; color: #777;}\n",
       "a {color: #8fa1b3;}\n",
       "\n",
       "/* INTRO PAGE */\n",
       ".toolbar_info, .list_container {color: #050505;}\n",
       "\n",
       "/* NOTEBOOK */\n",
       "\n",
       "/* comment out this line to bring the toolbar back */\n",
       "/*div#maintoolbar, div#header {display: none !important;}\n",
       "*/\n",
       "\n",
       "div.header {background-color: #050505;}\n",
       "div.lower-header-bar {background-color: #1b202b;}\n",
       "\n",
       "div#maintoolbar {background-color: #050505;}\n",
       "div#maintoolbar.navbar {background-color: #050505;}\n",
       "\n",
       "div#notebook {border-top: none; background-color: #050505;}\n",
       "div.navbar-collapse {background-color: #050505;}\n",
       "div#maintoolbar-container {display: none !important; background-color: 050505;}\n",
       "/*div#header-container {display: none !important;}\n",
       "*/\n",
       "\n",
       "div.input_prompt {color: #050505;}\n",
       "div.output_prompt {color: #050505;}\n",
       "div.input_area {\n",
       "  border-radius: 0px;\n",
       "  border: 1px solid #4f5b66;\n",
       "}\n",
       "div.output_area pre {font-weight: normal; color: #c0c5ce;}\n",
       "div.output_subarea {font-weight: normal; color: #c0c5ce;}\n",
       "\n",
       ".rendered_html table, .rendered_html th, .rendered_html tr, .rendered_html td {\n",
       "  border: 1px  #c0c5ce solid;\n",
       "  color: #c0c5ce;\n",
       "}\n",
       "div.output_html { font-family: sans-serif; }\n",
       "table.dataframe tr {border: 1px #c0c5ce;}\n",
       "\n",
       "div.cell.selected {border-radius: 0px;}\n",
       "div.cell.edit_mode {border-radius: 0px; border: thin solid #b48ead;}\n",
       "div.text_cell_render, div.output_html {color: #c0c5ce;}\n",
       "\n",
       "span.ansiblack {color: #343d46;}\n",
       "span.ansiblue {color: #96b5b4;}\n",
       "span.ansigray {color: #a7adba;}\n",
       "span.ansigreen {color: #a3be8c;}\n",
       "span.ansipurple {color: #b48ead;}\n",
       "span.ansired {color: #bf616a;}\n",
       "span.ansiyellow {color: #ebcb8b;}\n",
       "\n",
       "div.output_stderr {background-color: #050505;}\n",
       "div.output_stderr pre {color: #dfe1e8;}\n",
       "\n",
       ".cm-s-ipython.CodeMirror {background: #050505; color: #dfe1e8;}\n",
       ".cm-s-ipython div.CodeMirror-selected {background: #343d46 !important;}\n",
       ".cm-s-ipython .CodeMirror-gutters {background: #2b303b; border-right: 0px;}\n",
       ".cm-s-ipython .CodeMirror-linenumber {color: #65737e;}\n",
       ".cm-s-ipython .CodeMirror-cursor {border-left: 1px solid #a7adba !important;}\n",
       "\n",
       ".cm-s-ipython span.cm-comment {color: #ab7967;}\n",
       ".cm-s-ipython span.cm-atom {color: #b48ead;}\n",
       ".cm-s-ipython span.cm-number {color: #b48ead;}\n",
       "\n",
       ".cm-s-ipython span.cm-property, .cm-s-ipython span.cm-attribute {color: #a3be8c;}\n",
       ".cm-s-ipython span.cm-keyword {color: #bf616a;}\n",
       ".cm-s-ipython span.cm-string {color: #ebcb8b;}\n",
       ".cm-s-ipython span.cm-operator {color: #ab7967;}\n",
       ".cm-s-ipython span.cm-builtin {color: #b48ead;}\n",
       "\n",
       ".cm-s-ipython span.cm-variable {color: #a3be8c;}\n",
       ".cm-s-ipython span.cm-variable-2 {color: #8fa1b3;}\n",
       ".cm-s-ipython span.cm-def {color: #d08770;}\n",
       ".cm-s-ipython span.cm-error {background: #bf616a; color: #a7adba;}\n",
       ".cm-s-ipython span.cm-bracket {color: #c0c5ce;}\n",
       ".cm-s-ipython span.cm-tag {color: #bf616a;}\n",
       ".cm-s-ipython span.cm-link {color: #b48ead;}\n",
       "\n",
       ".cm-s-ipython .CodeMirror-matchingbracket { text-decoration: underline; color: #dfe1e8 !important;}\n",
       "\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def css_styling():\n",
    "    styles = open(\"/Users/jnoxon/Dropbox/python/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_code_strings(df, include):\n",
    "    count = 0\n",
    "    for i in df:\n",
    "        if i in include:        \n",
    "            dummies = pd.get_dummies(df[i])\n",
    "            for j in dummies:\n",
    "                name = j\n",
    "                df[name] = dummies[j]\n",
    "    t = df\n",
    "    for i in t:\n",
    "        if i in include:\n",
    "            t = t.drop(i, 1)\n",
    "    df = t\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"features-predictions-truths.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = one_hot_code_strings(df, ['artist', 'location', 'm1', 'm2', 'm3', 'm4'])\n",
    "df['exp_price_rating'] = np.exp(df['price_rating'])\n",
    "dfy = df['Actuals']\n",
    "# consider dropping price rating and exp price rating\n",
    "# for i in ['title', 'Location', 'Predictions', 'Actuals', 'price_rating', 'exp_price_rating']:\n",
    "for i in ['title', 'Location', 'Predictions', 'Actuals']:\n",
    "    df = df.drop(i,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.366560312004 linear_predictions\n",
      "6.58654357956 booster_predictions\n",
      "4.895 forest_predictions\n",
      "1.335 erf_predictions\n",
      "0.366560312004['linear_predictions']\n",
      "Time Elapsed = 0h 0m 20s\n"
     ]
    }
   ],
   "source": [
    "problem = apm(df, dfy, task='regression', hyperparameters='default')\n",
    "predictions = problem.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for i in xrange(len(predictions)):\n",
    "#     print predictions[i], problem.y_test[i]\n",
    "    \n",
    "# plt.scatter(np.log(predictions), np.log(problem.y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = sklearn.cross_validation.train_test_split(df, dfy, test_size = .20, random_state=42)\n",
    "forest = ensemble.ExtraTreesRegressor(n_estimators=1000)\n",
    "forest = forest.fit(x_train, y_train)\n",
    "forest_predictions = forest.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imp = []\n",
    "for i in xrange(len(x_train.columns)):\n",
    "    imp.append([x_train.columns[i], forest.feature_importances_[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# importances = pd.DataFrame(imp)\n",
    "# importances.sort_values(1, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in xrange(len(forest_predictions)):\n",
    "    print forest_predictions[i], np.array(y_test)[i]\n",
    "print np.median(np.abs(forest_predictions-np.array(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.99999999455124133, 9.9512854251048219e-205)\n",
      "0.645665952473\n",
      "0.366560312004\n",
      "449.70918408 450.0\n",
      "359.603364498 360.0\n",
      "119.641051387 120.0\n",
      "3069.12257527 3070.0\n",
      "404.652351372 405.0\n",
      "6138.72845009 6140.0\n",
      "1499.69621047 1500.0\n",
      "3069.22076807 3070.0\n",
      "449.59994226 450.0\n",
      "224.634939008 225.0\n",
      "540.685918922 540.0\n",
      "855.641044849 855.0\n",
      "139.648091671 140.0\n",
      "876.239333181 875.0\n",
      "8769.71347928 8770.0\n",
      "17999.6982806 18000.0\n",
      "449.677229122 450.0\n",
      "359.676360608 360.0\n",
      "525.202297339 525.0\n",
      "900.689304626 900.0\n",
      "199.685257608 200.0\n",
      "876.211235184 875.0\n",
      "630.709242164 630.0\n",
      "32229.163363 32230.0\n",
      "585.56894209 585.0\n",
      "700.707175796 700.0\n",
      "404.670884713 405.0\n",
      "675.642315148 675.0\n",
      "27716.6564737 27715.0\n",
      "449.700864138 450.0\n",
      "899.698615731 900.0\n",
      "2577.12324782 2580.0\n",
      "349.708941654 350.0\n",
      "37499.7941161 37500.0\n",
      "314.708328662 315.0\n",
      "1314.71021176 1315.0\n",
      "900.612817316 900.0\n",
      "19732.2507424 19730.0\n",
      "599.604393891 600.0\n",
      "495.646720955 495.0\n",
      "449.708490105 450.0\n",
      "540.700618095 540.0\n",
      "269.639907668 270.0\n",
      "449.707036278 450.0\n",
      "1081.61572488 1080.0\n",
      "399.641578919 400.0\n",
      "1799.63660959 1800.0\n",
      "449.638152821 450.0\n",
      "1351.70457926 1350.0\n",
      "788.692881522 790.0\n",
      "3499.61507423 3500.0\n",
      "4384.76051676 4385.0\n",
      "359.633439688 360.0\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats.stats import pearsonr   \n",
    "print np.mean(np.abs(predictions-problem.y_test))\n",
    "print np.median(np.abs((predictions-problem.y_test)))\n",
    "for i in xrange(len(predictions)):\n",
    "    print predictions[i], problem.y_test[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class apm():\n",
    "    def __init__(self, x, y, task, test=None, hyperparameters='individual', debug=False):\n",
    "        self.time = time.time()\n",
    "        self.task = task\n",
    "        self.hyperparameters = hyperparameters\n",
    "        self.x = np.array(x)\n",
    "        if self.task == 'classification':\n",
    "            self.y = np.array(y, dtype = int)\n",
    "            if type(self.y[0]) != int:\n",
    "                self.y, self.mapping = self.categorize(self.y)\n",
    "        elif self.task == 'regression':\n",
    "            self.y = np.array(y, dtype = float)\n",
    "        if test is None:\n",
    "            self.test = None\n",
    "        else:\n",
    "            self.test = np.array(test)            \n",
    "        self.x_train, self.x_test, self.y_train, self.y_test = sklearn.cross_validation.train_test_split\\\n",
    "            (self.x, self.y, test_size = .20, random_state=42)\n",
    "        self.x_train1 = self.x_train[:len(self.x_train)/2]\n",
    "        self.x_train2 = self.x_train[len(self.x_train)/2:]\n",
    "        self.y_train1 = self.y_train[:len(self.x_train)/2]\n",
    "        self.y_train2 = self.y_train[len(self.x_train)/2:]\n",
    "        self.stacking_train = self.x_train2\n",
    "        self.stacking_test = self.x_test\n",
    "        \n",
    "        self.x1 = self.x[:len(self.x)/2]\n",
    "        self.x2 = self.x[len(self.x)/2:]\n",
    "        self.y1 = self.y[:len(self.x)/2]\n",
    "        self.y2 = self.y[len(self.x)/2:]\n",
    "        self.test_stacking_train = self.x2\n",
    "        self.test_stacking_test = self.test\n",
    "\n",
    "        self.debug = False\n",
    "        self.stacking = False\n",
    "        self.testing = False\n",
    "        self.optimal_iterator = None\n",
    "        \n",
    "        if self.task == 'classification':\n",
    "            if self.debug:\n",
    "                self.algorithms = [self.logistic_regression]\n",
    "                self.algorithm_names = ['logistic_predictions']\n",
    "            else:\n",
    "                self.algorithms = [self.xgboost, self.gradient_booster, self.random_forest,\\\n",
    "                      self.extremely_random_forest, self.logistic_regression, self.lda, self.qda]\n",
    "                self.algorithm_names = ['xgboost_predictions','booster_predictions', 'forest_predictions', 'erf_predictions',\\\n",
    "                          'logistic_predictions', 'lda_predictions', 'qda_predictions', \\\n",
    "                           'xgboost_on_stack', 'booster_on_stack','forest_on_stack', \\\n",
    "                            'erf_on_stack', 'logistic_on_stack', 'lda_on_stack', 'qda_on_stack']\n",
    "        elif self.task == 'regression':\n",
    "            if self.debug:\n",
    "                self.algorithms = [self.linear_regression, self.gradient_booster, self.random_forest,\\\n",
    "                                  self.extremely_random_forest]\n",
    "                self.algorithm_names = ['linear_predictions','booster_predictions', \\\n",
    "                            'forest_predictions', 'erf_predictions', 'linear_on_stack',\\\n",
    "                                        'booster_on_stack', 'forest_on_stack', 'erf_on_stack']\n",
    "            else:\n",
    "                self.algorithms = [self.linear_regression, self.gradient_booster, self.random_forest,\\\n",
    "                                  self.extremely_random_forest]\n",
    "                self.algorithm_names = ['linear_predictions','booster_predictions', \\\n",
    "                            'forest_predictions', 'erf_predictions', 'linear_on_stack',\\\n",
    "                                        'booster_on_stack', 'forest_on_stack', 'erf_on_stack']\n",
    "                \n",
    "        self.xgb_max_depth = None\n",
    "        self.gb_n_estimators = None\n",
    "        self.gb_max_depth = None\n",
    "        self.rf_n_estimators = 400\n",
    "        self.rf_max_features = \"auto\"\n",
    "        self.erf_n_estimators = None\n",
    "        self.erf_max_features = None\n",
    "        self.log_reg_penalty = None\n",
    "        self.log_reg_C = None\n",
    "        self.qda_reg_param = None\n",
    "        self.svc_C = None\n",
    "        self.svc_kernel = None\n",
    "        \n",
    "        self.training_df = None\n",
    "        self.testing_df = None\n",
    "        \n",
    "        self.booster = None\n",
    "        \n",
    "        \n",
    "    \n",
    "    def accuracy(self, predictions):\n",
    "        if len(predictions) == len(self.y_test):\n",
    "            y_test = self.y_test\n",
    "        elif len(predictions) == len(self.y_train2):\n",
    "            y_test = self.y_train2\n",
    "        elif len(predictions) == len(self.y2):\n",
    "            y_test = self.y2\n",
    "        else:\n",
    "            print \"Accuracy called with incorrectly sized predictions\"\n",
    "        if self.task == 'classification':\n",
    "            count = 0\n",
    "            for i in xrange(len(predictions)):\n",
    "                if predictions[i] == y_test[i]:\n",
    "                    count += 1\n",
    "            accuracy = count/float(len(predictions))\n",
    "        elif self.task == 'regression':\n",
    "            error = [np.abs(predictions[i] - y_test[i]) for i in xrange(len(predictions))]\n",
    "            accuracy = np.median(error)\n",
    "        return accuracy\n",
    "    \n",
    "    def categorize(self, labels):\n",
    "        mapping = {}\n",
    "        labels = pd.Series(labels)\n",
    "        for i,v in enumerate(list(set(labels))):\n",
    "            labels = labels.replace(v, i)\n",
    "            mapping[i] = v\n",
    "        return np.array(labels), mapping  \n",
    "    \n",
    "    def stacker(self):\n",
    "        self.stacking = True\n",
    "        self.train_stacker()\n",
    "        self.stacking = False\n",
    "        return self.test_stacker()\n",
    "    \n",
    "    def time_elapsed(self):\n",
    "        seconds = time.time()-self.time\n",
    "        hours = int(seconds/3600)\n",
    "        seconds = seconds-hours*3600\n",
    "        minutes = int(seconds/60)\n",
    "        seconds = seconds - minutes*60\n",
    "        print \"Time Elapsed = %dh %dm %ds\" %(hours, minutes, seconds)\n",
    "        \n",
    "    def mean(self, l):\n",
    "        means = []\n",
    "        for i in xrange(len(l[0])):\n",
    "            s = 0\n",
    "            for j in l:\n",
    "                s += j[i]\n",
    "            means.append(float(s)/len(l))\n",
    "        return means\n",
    "    \n",
    "    def train_stacker(self):\n",
    "        for model in self.algorithms:\n",
    "            generate_train, generate_test = model()\n",
    "            if self.testing == False:\n",
    "                self.stacking_train = np.column_stack((self.stacking_train, generate_train))\n",
    "                self.stacking_test = np.column_stack((self.stacking_test, generate_test))\n",
    "            else:\n",
    "                self.test_stacking_train = np.column_stack((self.test_stacking_train, generate_train))\n",
    "                self.test_stacking_test = np.column_stack((self.test_stacking_test, generate_test))\n",
    "        return\n",
    "        \n",
    "    def test_stacker(self):\n",
    "        if self.testing == False:\n",
    "            return [model(self.stacking_train, self.y_train2, self.stacking_test, self.y_test) \\\n",
    "                for model in self.algorithms]\n",
    "        else:\n",
    "            return [model(self.test_stacking_train, self.y2, self.test_stacking_test, None)\\\n",
    "                for model in self.algorithms]\n",
    "            \n",
    "    def initialize_data(self, x_train=None, y_train=None, x_test=None, y_test=None):\n",
    "        if self.testing == False:\n",
    "            if self.stacking == False:\n",
    "                return self.x_train, self.y_train, self.x_test, self.y_test\n",
    "            else:\n",
    "                if x_train == None:\n",
    "                    x_train = self.x_train1\n",
    "                if y_train == None:\n",
    "                    y_train = self.y_train1\n",
    "                if x_test == None:\n",
    "                    x_test = self.x_train2\n",
    "                if y_test == None:\n",
    "                    y_test = self.y_train2\n",
    "                return x_train, y_train, x_test, y_test\n",
    "        else:\n",
    "            if self.stacking == False:\n",
    "                return self.x, self.y, self.test, None\n",
    "            else:\n",
    "                if x_train == None:\n",
    "                    x_train = self.x1\n",
    "                if y_train == None:\n",
    "                    y_train = self.y1\n",
    "                if x_test == None:\n",
    "                    x_test = self.x2\n",
    "                if y_test == None:\n",
    "                    y_test = self.y2\n",
    "                return x_train, y_train, x_test, y_test\n",
    "    \n",
    "    def stacking_test_data(self):\n",
    "        if self.testing == False:\n",
    "            return self.x_test\n",
    "        else:\n",
    "            return self.test\n",
    "        \n",
    "    def linear_regression(self, x_train=None, y_train=None, x_test=None, y_test=None):\n",
    "        x_train, y_train, x_test, y_test = self.initialize_data(x_train, y_train, x_test, y_test)\n",
    "        lr = linear_model.Lasso()\n",
    "        lr = lr.fit(x_train, y_train)\n",
    "        linear_predictions = lr.predict(x_test)\n",
    "        if self.stacking == True:\n",
    "            return linear_predictions, lr.predict(self.stacking_test_data())\n",
    "        return linear_predictions\n",
    "        \n",
    "    def xgboost(self, x_train=None, y_train=None, x_test=None, y_test=None):\n",
    "        x_train, y_train, x_test, y_test = self.initialize_data(x_train, y_train, x_test, y_test)\n",
    "        dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "        dtest = xgb.DMatrix(x_test)\n",
    "        param = {'bst:max_depth':50, 'bst:eta':1, 'silent':1,'objective':'multi:softmax'}\n",
    "        param['num_class'] = x_train.shape[1]\n",
    "#         evallist = [(dtest,'eval'), (dtrain,'train')]\n",
    "        num_round = 10\n",
    "        bst = xgb.train(param, dtrain, num_round, verbose_eval=False)\n",
    "        preds = bst.predict(dtest)\n",
    "#         for i in xrange(len(labels)):\n",
    "#             print labels[i], self.y_test[i]\n",
    "        if self.stacking == True:\n",
    "            return preds, bst.predict(xgb.DMatrix(self.stacking_test_data()))\n",
    "        return preds\n",
    "    \n",
    "    def gradient_booster(self, x_train=None, y_train=None, x_test=None, y_test=None):\n",
    "        x_train, y_train, x_test, y_test = self.initialize_data(x_train, y_train, x_test, y_test)\n",
    "        if self.hyperparameters == 'default':\n",
    "            if self.task == 'classification':\n",
    "                booster = ensemble.GradientBoostingClassifier(n_estimators = 400, max_depth = 2)\n",
    "            elif self.task == 'regression':\n",
    "                booster = ensemble.GradientBoostingRegressor(n_estimators = 400, max_depth = 2)\n",
    "            booster = booster.fit(x_train, y_train)\n",
    "            booster_predictions = booster.predict(x_test)\n",
    "            if self.stacking == True:\n",
    "                return booster_predictions, booster.predict(self.stacking_test_data())\n",
    "            return booster_predictions\n",
    "        elif self.hyperparameters == 'individual':\n",
    "            if self.gb_n_estimators == None:\n",
    "                max_accuracy = -1\n",
    "                for n_estimators in xrange(100, 1300, 300):\n",
    "                    if self.task == 'classification':\n",
    "                        booster = ensemble.GradientBoostingClassifier(n_estimators = n_estimators)\n",
    "                    elif self.task == 'regression':\n",
    "                        booster = ensemble.GradientBoostingRegressor(n_estimators = n_estimators)\n",
    "                    booster = booster.fit(x_train, y_train)\n",
    "                    booster_predictions = booster.predict(x_test)\n",
    "                    if self.accuracy(booster_predictions) > max_accuracy:\n",
    "                        max_accuracy = self.accuracy(booster_predictions)\n",
    "                        self.gb_n_estimators = n_estimators\n",
    "            if self.gb_max_depth == None:\n",
    "                max_accuracy = -1\n",
    "                for max_depth in xrange(2,15,3):\n",
    "                    if self.task == 'classification':\n",
    "                        booster = ensemble.GradientBoostingClassifier(max_depth = max_depth)\n",
    "                    elif self.task == 'regression':\n",
    "                        booster = ensemble.GradientBoostingRegressor(max_depth = max_depth)\n",
    "                    booster = booster.fit(x_train, y_train)\n",
    "                    booster_predictions = booster.predict(x_test)\n",
    "                    if self.accuracy(booster_predictions) > max_accuracy:\n",
    "                        max_accuracy = self.accuracy(booster_predictions)\n",
    "                        self.gb_max_depth = max_depth\n",
    "                print \"Optimal Booster Hyperparams: n_estimators = %d, max_depth = %d\" \\\n",
    "                        %(self.gb_n_estimators, self.gb_max_depth)\n",
    "            if self.task == 'classification':\n",
    "                booster = ensemble.GradientBoostingClassifier(\\\n",
    "                          n_estimators = self.gb_n_estimators,\n",
    "                          max_depth = self.gb_max_depth)\n",
    "            elif self.task == 'regression':\n",
    "                booster = ensemble.GradientBoostingRegressor(\\\n",
    "                          n_estimators = self.gb_n_estimators,\n",
    "                          max_depth = self.gb_max_depth)\n",
    "            booster = booster.fit(x_train, y_train)\n",
    "            booster_predictions = booster.predict(x_test)\n",
    "            if self.stacking == True:\n",
    "                return booster_predictions, booster.predict(self.stacking_test_data())\n",
    "            return booster_predictions\n",
    "        elif self.hyperparameters == 'grid':\n",
    "            max_accuracy = -1\n",
    "            for n_estimators in xrange(100, 1000, 300):\n",
    "                for max_depth in xrange(2,15,3):\n",
    "                    if self.task == 'classification':\n",
    "                        booster = ensemble.GradientBoostingClassifier\\\n",
    "                        (n_estimators = n_estimators, max_depth = max_depth)\n",
    "                    elif self.task == 'regression':\n",
    "                        booster = ensemble.GradientBoostingRegressor\\\n",
    "                        (n_estimators = n_estimators, max_depth = max_depth)\n",
    "                    booster = booster.fit(x_train, y_train)\n",
    "                    booster_predictions = booster.predict(x_test)\n",
    "                    print str(self.accuracy(booster_predictions)) + ' ' + str(n_estimators) + ' ' + str(max_depth)\n",
    "                    if self.accuracy(booster_predictions) > max_accuracy:\n",
    "                        max_accuracy = self.accuracy(booster_predictions)\n",
    "                        max_predictions = booster_predictions\n",
    "                        optimal_parameters = [n_estimators, max_depth]\n",
    "            print(\"Gradient booster: \", optimal_parameters)\n",
    "            return max_predictions\n",
    "                \n",
    "    def rf(self, x_train, y_train, x_test, n_estimators=1000, max_features=\"auto\"):\n",
    "        if self.task == 'classification':\n",
    "            forest = ensemble.RandomForestClassifier(n_estimators=n_estimators, max_features=max_features)\n",
    "            forest = forest.fit(x_train, y_train)\n",
    "            return forest.predict(x_test)\n",
    "        elif self.task == 'regression':\n",
    "            forest = ensemble.RandomForestRegressor(n_estimators=n_estimators, max_features=max_features)\n",
    "            forest = forest.fit(x_train, y_train)\n",
    "            return forest.predict(x_test)            \n",
    "        \n",
    "    \n",
    "    def random_forest(self, x_train=None, y_train=None, x_test=None, y_test=None):\n",
    "        x_train, y_train, x_test, y_test = self.initialize_data(x_train, y_train, x_test, y_test)\n",
    "        if self.hyperparameters == 'default':\n",
    "            if self.stacking == True:\n",
    "                return self.rf(x_train, y_train, x_test), self.rf(x_train, y_train, self.stacking_test_data())\n",
    "            return self.rf(x_train, y_train, x_test)\n",
    "        elif self.hyperparameters == 'individual':\n",
    "            if self.rf_n_estimators == None:\n",
    "                max_accuracy = -1\n",
    "                for n_estimators in xrange(100, 1400, 400):\n",
    "                    forest_predictions = self.rf(x_train, y_train, x_test, n_estimators=n_estimators)\n",
    "                    if self.accuracy(forest_predictions) > max_accuracy:\n",
    "                        max_accuracy = self.accuracy(forest_predictions)\n",
    "                        self.rf_n_estimators = n_estimators\n",
    "            if self.rf_max_features == None:\n",
    "                feat = x_train.shape[1]\n",
    "                max_accuracy = -1\n",
    "                for max_features in xrange(int(.3*feat), int(.6*feat), int(.05*feat)+1):\n",
    "                    print max_features, feat\n",
    "                    if max_features == 0:\n",
    "                        max_features = 1\n",
    "                    forest_predictions = self.rf(x_train, y_train, x_test, max_features=max_features)\n",
    "                    if self.accuracy(forest_predictions) > max_accuracy:\n",
    "                        max_accuracy = self.accuracy(forest_predictions)\n",
    "                        self.rf_max_features = max_features\n",
    "                print \"Optimal Forest Hyperparams: n_estimators = %d, max_features = %d\" \\\n",
    "                        %(self.rf_n_estimators, self.rf_max_features)\n",
    "            forest_predictions = self.rf(x_train, y_train, x_test,\\\n",
    "                 n_estimators=self.rf_n_estimators, max_features = self.rf_max_features)\n",
    "            if self.stacking == True:\n",
    "                return forest_predictions, self.rf(x_train, y_train, self.stacking_test_data(),\\\n",
    "                                                   self.rf_n_estimators, self.rf_max_features)\n",
    "            return forest_predictions \n",
    "        elif self.hyperparameters == 'grid':\n",
    "            feat = x_train.shape[1]\n",
    "            max_accuracy = -1\n",
    "            for n_estimators in xrange(100, 1000, 200):\n",
    "                for max_features in xrange(int(.3*feat), int(.6*feat), int(.03*feat)+1):\n",
    "                    if max_features == 0:\n",
    "                        max_features = 1\n",
    "                    for criterion in ['gini', 'entropy']:\n",
    "                        forest_predictions = self.rf(x_train, y_train, x_test, n_estimators = \\\n",
    "                                 n_estimators, criterion = criterion, max_features = max_features)\n",
    "                        print self.accuracy(forest_predictions)\n",
    "                        if self.accuracy(forest_predictions) > max_accuracy:\n",
    "                            max_accuracy = self.accuracy(forest_predictions)\n",
    "                            max_predictions = forest_predictions\n",
    "                            optimal_parameters = [n_estimators, max_features, criterion]\n",
    "            print[\"Random forest: \", optimal_parameters]\n",
    "            return max_predictions\n",
    "\n",
    "    def erf(self, x_train, y_train, x_test, n_estimators=1000, max_features=\"auto\"):\n",
    "        if self.task == 'classification':\n",
    "            forest = ensemble.ExtraTreesClassifier(n_estimators=n_estimators, max_features=max_features)\n",
    "            forest = forest.fit(x_train, y_train)\n",
    "            return forest.predict(x_test)\n",
    "        elif self.task == 'regression':\n",
    "            forest = ensemble.ExtraTreesRegressor(n_estimators=n_estimators, max_features=max_features)\n",
    "            forest = forest.fit(x_train, y_train)\n",
    "            return forest.predict(x_test)         \n",
    "    \n",
    "    def extremely_random_forest(self, x_train=None, y_train=None, x_test=None, y_test=None):\n",
    "        x_train, y_train, x_test, y_test = self.initialize_data(x_train, y_train, x_test, y_test)\n",
    "        if self.hyperparameters == 'default':\n",
    "            if self.stacking == True:\n",
    "                return self.erf(x_train, y_train, x_test), self.erf(x_train, y_train, self.stacking_test_data())\n",
    "            return self.erf(x_train, y_train, x_test)\n",
    "        elif self.hyperparameters == 'individual':\n",
    "            if self.erf_n_estimators == None:\n",
    "                max_accuracy = -1\n",
    "                for n_estimators in xrange(100, 1300, 400):\n",
    "                    erf_predictions = self.erf(x_train, y_train, x_test, n_estimators = n_estimators)\n",
    "                    if self.accuracy(erf_predictions) > max_accuracy:\n",
    "                        max_accuracy = self.accuracy(erf_predictions)\n",
    "                        self.erf_n_estimators = n_estimators\n",
    "            if self.erf_max_features == None:\n",
    "                feat = x_train.shape[1]\n",
    "                max_accuracy = -1\n",
    "                for max_features in xrange(int(.3*feat), int(.6*feat), int(.05*feat)+1):\n",
    "                    erf_predictions = self.erf(x_train, y_train, x_test, max_features=max_features)\n",
    "                    if self.accuracy(erf_predictions) > max_accuracy:\n",
    "                        max_accuracy = self.accuracy(erf_predictions)\n",
    "                        self.erf_max_features = max_features\n",
    "                print \"Optimal erf Hyperparams: n_estimators = %d, max_features = %d\" \\\n",
    "                        %(self.erf_n_estimators, self.erf_max_features)\n",
    "            erf_predictions = self.erf(x_train, y_train, x_test,\\\n",
    "                 n_estimators=self.erf_n_estimators, max_features = self.erf_max_features)\n",
    "            if self.stacking == True:\n",
    "                return erf_predictions, self.erf(x_train, y_train, x_test,\\\n",
    "                         n_estimators=self.erf_n_estimators, max_features = self.erf_max_features)\n",
    "            return erf_predictions\n",
    "    \n",
    "    def logistic_regression(self, x_train=None, y_train=None, x_test=None, y_test=None):\n",
    "        x_train, y_train, x_test, y_test = self.initialize_data(x_train, y_train, x_test, y_test)\n",
    "        if self.hyperparameters == 'default':\n",
    "            logistic = linear_model.LogisticRegression(penalty=\"l2\", C=1)#class_weight='auto'\n",
    "            logistic.fit(x_train, y_train)\n",
    "            logistic_predictions = logistic.predict(x_test)\n",
    "            if self.stacking == True:\n",
    "                return logistic_predictions, logistic.predict(self.stacking_test_data())\n",
    "            return logistic_predictions\n",
    "        elif self.hyperparameters == 'individual':\n",
    "            if self.log_reg_penalty == None:\n",
    "                for penalty in ['l1', 'l2']:\n",
    "                    if penalty == 'l1':\n",
    "                        logistic = linear_model.LogisticRegression(penalty=\"l1\")#class_weight='auto'\n",
    "                        logistic.fit(x_train, y_train)\n",
    "                        logistic_predictions = logistic.predict(x_test)\n",
    "                        l1_accuracy = self.accuracy(logistic_predictions)\n",
    "                        self.log_reg_penalty = 'l1'\n",
    "                    else:\n",
    "                        logistic = linear_model.LogisticRegression(penalty=\"l2\")#class_weight='auto'\n",
    "                        logistic.fit(x_train, y_train)\n",
    "                        logistic_predictions = logistic.predict(x_test)\n",
    "                        if self.accuracy(logistic_predictions) > l1_accuracy:\n",
    "                            self.log_reg_penalty = 'l2'\n",
    "            if self.log_reg_C == None:\n",
    "                max_accuracy = -1\n",
    "                for C in xrange(1,104,4):\n",
    "                    logistic = logistic = linear_model.LogisticRegression(penalty=self.log_reg_penalty, C=C)\n",
    "                    logistic.fit(x_train, y_train)\n",
    "                    logistic_predictions = logistic.predict(x_test)\n",
    "                    if self.accuracy(logistic_predictions) > max_accuracy:\n",
    "                        max_accuracy = self.accuracy(logistic_predictions)\n",
    "                        self.log_reg_C = C\n",
    "                print \"Optimal Logistic Regression Hyperparams: Penalty = %s, C = %d.\"\\\n",
    "                    %(self.log_reg_penalty, self.log_reg_C)\n",
    "            logistic = linear_model.LogisticRegression(penalty=self.log_reg_penalty, C = self.log_reg_C)#class_weight='auto'\n",
    "            logistic.fit(x_train, y_train)\n",
    "            logistic_predictions = logistic.predict(x_test)\n",
    "            if self.stacking == True:\n",
    "                return logistic_predictions, logistic.predict(self.stacking_test_data())\n",
    "            return logistic_predictions\n",
    "    \n",
    "    def lda(self, x_train=None, y_train=None, x_test=None, y_test=None):\n",
    "        x_train, y_train, x_test, y_test = self.initialize_data(x_train, y_train, x_test, y_test)\n",
    "        lda = discriminant_analysis.LinearDiscriminantAnalysis()\n",
    "        lda.fit(x_train, y_train)\n",
    "        lda_predictions = lda.predict(x_test)\n",
    "        if self.stacking == True:\n",
    "            return lda_predictions, lda.predict(self.stacking_test_data())\n",
    "        return lda_predictions\n",
    "    \n",
    "    def qda(self, x_train=None, y_train=None, x_test=None, y_test=None):\n",
    "        x_train, y_train, x_test, y_test = self.initialize_data(x_train, y_train, x_test, y_test)\n",
    "        if self.hyperparameters == 'default':\n",
    "            qda = discriminant_analysis.QuadraticDiscriminantAnalysis()\n",
    "            qda.fit(x_train, y_train)\n",
    "            qda_predictions = qda.predict(x_test)\n",
    "            return qda_predictions\n",
    "        elif self.hyperparameters == 'individual':\n",
    "            if self.qda_reg_param == None:\n",
    "                max_accuracy = -1\n",
    "                for reg_param in [x*.1 for x in range(0,10,2)]:\n",
    "                    qda = discriminant_analysis.QuadraticDiscriminantAnalysis(reg_param = reg_param)\n",
    "                    qda.fit(x_train, y_train)\n",
    "                    qda_predictions = qda.predict(x_test)\n",
    "                    if self.accuracy(qda_predictions) > max_accuracy:\n",
    "                        max_accuracy = self.accuracy(qda_predictions)\n",
    "                        self.qda_reg_param = reg_param\n",
    "                print \"Optimal QDA Hyperparams: Reg_param = %d\" %(self.qda_reg_param)\n",
    "            qda = discriminant_analysis.QuadraticDiscriminantAnalysis(reg_param = self.qda_reg_param)\n",
    "            qda.fit(x_train, y_train)\n",
    "            qda_predictions = qda.predict(x_test)\n",
    "            if self.stacking == True:\n",
    "                return qda_predictions, qda.predict(self.stacking_test_data())\n",
    "            return qda_predictions\n",
    "    \n",
    "    def svc(self, x_train=None, y_train=None, x_test=None, y_test=None):\n",
    "        x_train, y_train, x_test, y_test = self.initialize_data(x_train, y_train, x_test, y_test)\n",
    "        if self.hyperparameters == 'default':\n",
    "            support_vm = svm.SVC()\n",
    "            support_vm.fit(x_train, y_train)\n",
    "            svm_predictions = support_vm.predict(x_test)\n",
    "            return svm_predictions\n",
    "        elif self.hyperparameters == 'individual':\n",
    "            if self.svc_C == None:\n",
    "                max_accuracy = -1\n",
    "                for C in [10, 100]:\n",
    "                    s = svm.SVC(C=C)\n",
    "                    s.fit(x_train, y_train)\n",
    "                    svm_predictions = s.predict(x_test)\n",
    "                    if self.accuracy(svm_predictions) > max_accuracy:\n",
    "                        max_accuracy = self.accuracy(svm_predictions)\n",
    "                        self.svc_C = C\n",
    "            if self.svc_kernel == None:\n",
    "                max_accuracy = -1\n",
    "                for kernel in ['rbf', 'linear', 'poly']:\n",
    "                    s = svm.SVC(C = self.svc_C, kernel=kernel)\n",
    "                    s.fit(x_train, y_train)\n",
    "                    svm_predictions = s.predict(x_test)\n",
    "                    if self.accuracy(svm_predictions) > max_accuracy:\n",
    "                        max_accuracy = self.accuracy(svm_predictions)\n",
    "                        self.svc_kernel = kernel\n",
    "                print \"Optimal SVM Hyperparams: Kernel = %s, C = %d\" %(self.svc_kernel, self.svc_C)\n",
    "            s = svm.SVC(C = self.svc_C, kernel=self.svc_kernel)\n",
    "            s.fit(x_train, y_train)\n",
    "            svm_predictions = s.predict(x_test)\n",
    "            if self.stacking == True:\n",
    "                return svm_predictions, s.predict(self.stacking_test_data())\n",
    "            return svm_predictions\n",
    "    \n",
    "    def blend(self, args):\n",
    "        blended_predictions = []\n",
    "        if self.task == 'classification':\n",
    "            n = len(args)\n",
    "            for i in xrange(len(args[0])):\n",
    "                counts = {}\n",
    "                for j in xrange(n):\n",
    "                    try:\n",
    "                        counts[args[j][i]] += 1\n",
    "                    except KeyError:\n",
    "                        counts[args[j][i]] = 1\n",
    "                maximus = -float('inf')\n",
    "                for index, value in counts.iteritems():\n",
    "                    if value > maximus:\n",
    "                        max_value = index\n",
    "                        maximus = value\n",
    "                blended_predictions.append(max_value)\n",
    "        elif self.task == 'regression':\n",
    "            blended_predictions = self.mean(args)\n",
    "        return blended_predictions\n",
    "    \n",
    "    def run_algorithms(self):\n",
    "        if self.task == 'classification':\n",
    "            return [i() for i in self.algorithms]\n",
    "        elif self.task == 'regression':\n",
    "            return [i() for i in self.algorithms]\n",
    "    \n",
    "    def create_predictions(self):\n",
    "        self.testing = True\n",
    "        predictions = (self.run_algorithms() + self.stacker())\n",
    "        self.testing = False\n",
    "        \n",
    "        predictions_df = pd.DataFrame(predictions).transpose()\n",
    "        self.testing_df = predictions_df\n",
    "        predictions_df.columns = self.algorithm_names\n",
    "        \n",
    "        optimal_predictions = self.blend([predictions_df[j].values for j in self.optimal_iterator])\n",
    "        self.time_elapsed()\n",
    "        return optimal_predictions\n",
    "    \n",
    "    def predict(self):\n",
    "        algorithms = self.run_algorithms()\n",
    "\n",
    "        for i, v in enumerate(algorithms):\n",
    "        \n",
    "            print self.accuracy(v), self.algorithm_names[i]\n",
    "        \n",
    "        predictions = (algorithms + self.stacker())\n",
    "        \n",
    "        predictions_df = pd.DataFrame(predictions).transpose()\n",
    "        self.training_df = predictions_df\n",
    "        predictions_df.columns = self.algorithm_names\n",
    "\n",
    "        combinations = []\n",
    "        col = predictions_df.columns\n",
    "        for i in xrange(1, len(col), 2):\n",
    "            for j in itertools.combinations(col, i):\n",
    "                combinations.append([k for k in j])\n",
    "                \n",
    "        if self.task == 'classification':\n",
    "            maximus = -1\n",
    "            for i in combinations:\n",
    "                blended = self.blend([predictions_df[j].values for j in i])\n",
    "                accuracy = self.accuracy(blended)\n",
    "                if accuracy > maximus:\n",
    "                    maximus = accuracy\n",
    "                    optimal_predictions = blended\n",
    "                    self.optimal_iterator = i\n",
    "                    print str(accuracy) + str(i)            \n",
    "        \n",
    "        if self.task == 'regression':\n",
    "            maximus = float(\"inf\")\n",
    "            for i in combinations:\n",
    "                blended = self.blend([predictions_df[j].values for j in i])\n",
    "                accuracy = self.accuracy(blended)\n",
    "                if accuracy < maximus:\n",
    "                    maximus = accuracy\n",
    "                    optimal_predictions = blended\n",
    "                    self.optimal_iterator = i\n",
    "                    print str(accuracy) + str(i)\n",
    "        \n",
    "        if self.test != None: \n",
    "            return self.create_predictions()\n",
    "        else:\n",
    "            self.time_elapsed()\n",
    "            return optimal_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
